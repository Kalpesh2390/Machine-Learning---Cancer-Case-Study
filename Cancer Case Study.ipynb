{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer prediction case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the required packages.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file using panda\n",
    "\n",
    "path = 'C:/Users/E056362/Documents/New Project/Machine Learning Class - Assignment/Cancer Case Study/cancer.csv'\n",
    "CancerDF = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      "id                   569 non-null int64\n",
      "diagnosis            569 non-null object\n",
      "radius_mean          569 non-null float64\n",
      "texture_mean         569 non-null float64\n",
      "perimeter_mean       569 non-null float64\n",
      "area_mean            569 non-null float64\n",
      "smoothness_mean      569 non-null float64\n",
      "compactness_mean     569 non-null float64\n",
      "concavity_mean       569 non-null float64\n",
      "points_mean          569 non-null float64\n",
      "symmetry_mean        569 non-null float64\n",
      "dimension_mean       569 non-null float64\n",
      "radius_se            569 non-null float64\n",
      "texture_se           569 non-null float64\n",
      "perimeter_se         569 non-null float64\n",
      "area_se              569 non-null float64\n",
      "smoothness_se        569 non-null float64\n",
      "compactness_se       569 non-null float64\n",
      "concavity_se         569 non-null float64\n",
      "points_se            569 non-null float64\n",
      "symmetry_se          569 non-null float64\n",
      "dimension_se         569 non-null float64\n",
      "radius_worst         569 non-null float64\n",
      "texture_worst        569 non-null float64\n",
      "perimeter_worst      569 non-null float64\n",
      "area_worst           569 non-null float64\n",
      "smoothness_worst     569 non-null float64\n",
      "compactness_worst    569 non-null float64\n",
      "concavity_worst      569 non-null float64\n",
      "points_worst         569 non-null float64\n",
      "symmetry_worst       569 non-null float64\n",
      "dimension_worst      569 non-null float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Take a frist look at dataset\n",
    "CancerDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations.\n",
    "\n",
    "1. Dataset having total 32 columns\n",
    "2. Each column as 569 --> This means there are no missing entries\n",
    "3. Except Daignosis all other columns are float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87139402</td>\n",
       "      <td>B</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8910251</td>\n",
       "      <td>B</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905520</td>\n",
       "      <td>B</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>868871</td>\n",
       "      <td>B</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9012568</td>\n",
       "      <td>B</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0  87139402         B        12.32         12.39           78.85      464.1   \n",
       "1   8910251         B        10.60         18.95           69.28      346.4   \n",
       "2    905520         B        11.04         16.83           70.92      373.2   \n",
       "3    868871         B        11.28         13.39           73.00      384.8   \n",
       "4   9012568         B        15.19         13.21           97.65      711.8   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  points_mean  \\\n",
       "0          0.10280           0.06981         0.03987      0.03700   \n",
       "1          0.09688           0.11470         0.06387      0.02642   \n",
       "2          0.10770           0.07804         0.03046      0.02480   \n",
       "3          0.11640           0.11360         0.04635      0.04796   \n",
       "4          0.07963           0.06934         0.03393      0.02657   \n",
       "\n",
       "        ...         radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0       ...                13.50          15.64            86.97       549.1   \n",
       "1       ...                11.88          22.94            78.28       424.8   \n",
       "2       ...                12.41          26.44            79.93       471.4   \n",
       "3       ...                11.92          15.77            76.53       434.0   \n",
       "4       ...                16.20          15.73           104.50       819.1   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  points_worst  \\\n",
       "0            0.1385             0.1266          0.12420       0.09391   \n",
       "1            0.1213             0.2515          0.19160       0.07926   \n",
       "2            0.1369             0.1482          0.10670       0.07431   \n",
       "3            0.1367             0.1822          0.08669       0.08611   \n",
       "4            0.1126             0.1737          0.13620       0.08178   \n",
       "\n",
       "   symmetry_worst  dimension_worst  \n",
       "0          0.2827          0.06771  \n",
       "1          0.2940          0.07587  \n",
       "2          0.2998          0.07881  \n",
       "3          0.2102          0.06784  \n",
       "4          0.2487          0.06766  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take look at values of first 5 rows\n",
    "CancerDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'M'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Unique values from column Diagnosis\n",
    "CancerDF.diagnosis.unique()\n",
    "# So there are 2 Unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Labels and features into Numpy array\n",
    "label = CancerDF.diagnosis.values\n",
    "features = CancerDF.iloc[:,2:32].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Train Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                label,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogModel = LogisticRegression()\n",
    "LogModel.fit(X_train,y_train)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9582417582417583\n",
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "# Check the Accuracy of Model\n",
    "print(LogModel.score(X_train,y_train))\n",
    "print(LogModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 1\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 2\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 9\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 14\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 15\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 16\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9494505494505494 seed 19\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 21\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 22\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9582417582417583 seed 25\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 32\n",
      "0.9824561403508771\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 33\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 35\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 36\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 40\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 42\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 43\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9824561403508771 Train 0.9516483516483516 seed 45\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 50\n",
      "0.9824561403508771\n",
      "Test 0.9824561403508771 Train 0.945054945054945 seed 51\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 54\n",
      "0.9824561403508771\n",
      "0.9824561403508771\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 56\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 57\n",
      "0.9824561403508771\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 58\n",
      "0.9824561403508771\n",
      "Test 0.9912280701754386 Train 0.9494505494505494 seed 59\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 63\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 64\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 65\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 66\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 68\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 69\n",
      "0.9912280701754386\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 70\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9472527472527472 seed 72\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 74\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 77\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 79\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 80\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 81\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 82\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 87\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 88\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 91\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 92\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 95\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9516483516483516 seed 96\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 98\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 104\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 105\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 107\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 110\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 112\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 114\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9560439560439561 seed 116\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 118\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9604395604395605 seed 119\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9582417582417583 seed 123\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 126\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 130\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9912280701754386 Train 0.9494505494505494 seed 136\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9604395604395605 seed 142\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9582417582417583 seed 143\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 145\n",
      "0.9912280701754386\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 146\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 149\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 150\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9582417582417583 seed 151\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "0.9912280701754386\n",
      "Test 0.9736842105263158 Train 0.9560439560439561 seed 154\n",
      "0.9912280701754386\n",
      "Test 1.0 Train 0.9494505494505494 seed 155\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 160\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9472527472527472 seed 161\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9582417582417583 seed 162\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 166\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 167\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 168\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9516483516483516 seed 171\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 180\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 181\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 184\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9494505494505494 seed 187\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9604395604395605 seed 189\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 192\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 195\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9472527472527472 seed 196\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 198\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 204\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 206\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9582417582417583 seed 211\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 212\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 213\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 215\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 216\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 217\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 220\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 224\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 230\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 233\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.945054945054945 seed 247\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 249\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9516483516483516 seed 250\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9494505494505494 seed 253\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 257\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 260\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 263\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 265\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 266\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 270\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 272\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 273\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 274\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 277\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9516483516483516 seed 278\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 285\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 298\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9560439560439561 seed 300\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 301\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9494505494505494 seed 305\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 310\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 314\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9538461538461539 seed 315\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 317\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 318\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9648351648351648 seed 319\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 320\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 321\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.945054945054945 seed 322\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 330\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 332\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 333\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 334\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 335\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9582417582417583 seed 339\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9604395604395605 seed 343\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 345\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9472527472527472 seed 346\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9538461538461539 seed 351\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9560439560439561 seed 353\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9516483516483516 seed 354\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9516483516483516 seed 361\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 362\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 363\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 364\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9912280701754386 Train 0.9516483516483516 seed 366\n",
      "1.0\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9560439560439561 seed 368\n",
      "1.0\n",
      "Test 0.956140350877193 Train 0.9494505494505494 seed 369\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9560439560439561 seed 372\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9824561403508771 Train 0.9494505494505494 seed 376\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9538461538461539 seed 378\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9582417582417583 seed 380\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9560439560439561 seed 385\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9736842105263158 Train 0.9538461538461539 seed 392\n",
      "1.0\n",
      "1.0\n",
      "Test 0.9649122807017544 Train 0.9494505494505494 seed 394\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Fine Tune the Random state of the model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "HighScore = 0\n",
    "for i in range(0,401):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                label,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=i)\n",
    "    LogModel = LogisticRegression()\n",
    "    LogModel.fit(X_train,y_train)\n",
    "    TrainScore = LogModel.score(X_train,y_train)\n",
    "    TestScore  = LogModel.score(X_test,y_test)\n",
    "    if (TestScore > TrainScore):\n",
    "        print('Test {} Train {} seed {}'.format(TestScore,TrainScore,i))\n",
    "        if ( HighScore < TestScore):\n",
    "            HighScore = TestScore\n",
    "        \n",
    "        \n",
    "    print(HighScore)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOr seed 155 we are getting highest test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9494505494505494\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Test Train Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                label,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=155)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogModel = LogisticRegression()\n",
    "LogModel.fit(X_train,y_train)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Check the Accuracy of Model\n",
    "print(LogModel.score(X_train,y_train))\n",
    "print(LogModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[347,  10],\n",
       "       [ 13, 199]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(label,LogModel.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.96      0.97      0.97       357\n",
      "           M       0.95      0.94      0.95       212\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       569\n",
      "   macro avg       0.96      0.96      0.96       569\n",
      "weighted avg       0.96      0.96      0.96       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label,LogModel.predict(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Score = Pricision of B + Recall of M / 2 = 0.96 + 0.94 /2 = 0.95 \n",
    "# So we deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# This is Unbalanced Dataset So we will draw PR curve for it\n",
    "# Before we go ahead we need to Binarize our Lable.\n",
    "# Lets check if our model supoorts either predict proba or decision functin\n",
    "\n",
    "# Step 1: encode the labels.\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y = label_binarize(label,classes=['B','M'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                y,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create LogisticRegression OneVsRestClassifier Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "multiClassModel = OneVsRestClassifier(LogisticRegression())\n",
    "y_score = multiClassModel.fit(X_train,y_train).decision_function(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25b9e1b12b0>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot PR\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "precision,recall,_ = precision_recall_curve(y,y_score)\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0.5,0.5], linestyle = '--')\n",
    "plt.plot(recall,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
